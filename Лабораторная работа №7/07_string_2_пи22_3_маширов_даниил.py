# -*- coding: utf-8 -*-
"""07_string_2_ПИ22-3_Маширов_Даниил.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wrtw_OJ7woVZHrzz1iJlSpHw0UUi-thw

# Введение в обработку текста на естественном языке

Материалы:
* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\
* https://realpython.com/nltk-nlp-python/
* https://scikit-learn.org/stable/modules/feature_extraction.html

## Задачи для совместного разбора
"""

pip install pymorphy2

from sklearn.feature_extraction.text import CountVectorizer
import pymorphy2

"""1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. """

from nltk.metrics.distance import edit_distance
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
import random
import pandas as pd

import nltk
nltk.download('punkt')

words = []
with open('litw-win.txt', 'r', encoding='windows-1251') as f:
    for line in f:
        words.append(line.split()[1])

text = "с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий"

for token in word_tokenize(text, language="russian"):
    if token not in words:
        distances = [edit_distance(token, word) for word in words]
        closest_word = words[distances.index(min(distances))]
        print(closest_word, end=' ')
    else:
        print(token, end=' ')

"""2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."""

text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'

stemmer = SnowballStemmer(language="russian")
morph = pymorphy2.MorphAnalyzer()

tokenized_text = word_tokenize(text, language="russian")
stemmed_words = [stemmer.stem(word) for word in tokenized_text]
normalized_words = [morph.parse(word)[0].normal_form for word in tokenized_text]

print(stemmed_words)
print(normalized_words)

"""3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."""

text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'

sentences = sent_tokenize(text, language="russian")

vectorizer = CountVectorizer()
vectorizer.fit(sentences)  # составляет словарь
vector = vectorizer.transform(sentences)
vector_array = vector.toarray()

vector_array

"""## Лабораторная работа 9

### Расстояние редактирования

1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`).
"""

descriptions_df = pd.read_csv('preprocessed_descriptions.csv', index_col='Unnamed: 0')
list_descriptions = descriptions_df['preprocessed_descriptions'].to_list()

words = list(set(word for description in list_descriptions for word in word_tokenize(str(description))))

print(words)

"""1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."""

import random
examples = list(zip(random.sample(words, 5), random.sample(words, 5)))
for i,j in examples:
  print(i, j, '-->', edit_distance(i, j))

"""1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"""

def find_nearest_words(word, k):
    global words
    word_distances = [(edit_distance(word, i), i) for i in words]
    sorted_distances = sorted(word_distances)[:k+1]
    return sorted_distances

print(find_nearest_words('cambodiansed', 3))

"""### Стемминг, лемматизация"""

import nltk
nltk.download('wordnet')

"""2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: 
    * word
    * stemmed_word 
    * normalized_word 

Столбец `word` укажите в качестве индекса. 

Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации.
"""

from nltk.stem import WordNetLemmatizer

# Создание экземпляров лемматизатора и стеммера
wnl = WordNetLemmatizer()
sb = SnowballStemmer(language="english")

# Создание списка слов
words = list(set(k for j in [word_tokenize(str(i)) for i in list_descriptions] for k in j))

# Создание таблицы с обработанными словами
table = pd.DataFrame({
    'word': words,
    'stemmed_word': [sb.stem(i) for i in words],
    'normalized_word': [wnl.lemmatize(i) for i in words]
})

# Установка индекса таблицы
table.set_index('word')

"""2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."""

from nltk.corpus import stopwords 
 nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import collections

# Чтение данных из CSV-файла
descript = pd.read_csv('preprocessed_descriptions.csv', index_col='Unnamed: 0')
list_descript = descript['preprocessed_descriptions'].tolist()

# Токенизация и создание списка всех слов
words = [k for j in [word_tokenize(str(i)) for i in list_descript] for k in j]

# Удаление стоп-слов
stop_words = set(stopwords.words('english'))
words_ = [word for word in words[:100] if word not in stop_words]

# Подсчет статистики
total_words = len(words)
filtered_words = len(words_)
stopwords_ratio = (total_words - filtered_words) / total_words

counter1 = collections.Counter(words)
counter2 = collections.Counter(words_)

# Вывод результатов
print('Всего слов:', total_words)
print('Слов без стоп-слов:', filtered_words)
print('Доля стоп-слов:', stopwords_ratio)
print('Топ-5 слов до удаления:', counter1.most_common(5))
print('Топ-5 слов после удаления:', counter2.most_common(5))

"""### Векторное представление текста

3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Чтение данных из CSV-файла
descript = pd.read_csv('preprocessed_descriptions.csv', index_col='Unnamed: 0')

# Выбор случайных 5 примеров
samples = descript.sample(5)
five_samples = samples['preprocessed_descriptions'].tolist()

# Создание векторизатора
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')

# Преобразование текста в векторное представление
vectors = vectorizer.fit_transform(five_samples).toarray()

# Вывод вектора для первого примера
vectors[0]

"""3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."""

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.spatial import distance

# Чтение данных из CSV-файла
descript = pd.read_csv('preprocessed_descriptions.csv', index_col='Unnamed: 0')

# Выбор случайных 5 примеров
samples = descript.sample(5)
five_samples = samples['preprocessed_descriptions'].tolist()

# Создание векторизатора
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')

# Преобразование текста в векторное представление
vectors = vectorizer.fit_transform(five_samples).toarray()

# Получение имён примеров
names = samples['name'].tolist()

# Вычисление косинусного расстояния между векторами
distances = [[distance.cosine(vectors[names.index(i)], vectors[j]) for j in range(5)] for i in names]

# Создание DataFrame с результатами
data = pd.DataFrame(distances, columns=names, index=names)

# Вывод DataFrame
data